{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import urllib\n",
    "from dragin import AttnWeightRAG, model\n",
    "import nltk\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from groq import Groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GROQ_API_KEY'] = \"YOUR-GROQ-API-KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_curl_request(query, k=2):\n",
    "    # URL encode the query\n",
    "    encoded_query = urllib.parse.quote(query)\n",
    "    url = f\"http://localhost:8000/v1/retrieve?query={encoded_query}&k={k}\"\n",
    "    \n",
    "    # Construct the curl command\n",
    "    command = [\n",
    "        \"curl\", \"-X\", \"GET\", url, \"-H\", \"accept: */*\"\n",
    "    ]\n",
    "    \n",
    "    # Execute the command\n",
    "    try:\n",
    "        result = subprocess.run(command, capture_output=True, text=True, check=True)\n",
    "        # print(\"Response:\\n\", result.stdout)\n",
    "        return result.stdout\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Error:\", e.stderr)\n",
    "        return e.stderr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(text):\n",
    "    return ''.join([char for char in text if ord(char) < 128])\n",
    "\n",
    "def retrieve(self, query, topk=1, max_query_length=64):\n",
    "    self.counter.retrieve += 1\n",
    "    docs = execute_curl_request(query=query, k=topk)\n",
    "    docs = [remove_non_ascii(doc['text']) for doc in eval(docs.replace(\"null\", \"None\"))]\n",
    "    return docs\n",
    "model.retrieve = retrieve.__get__(model, AttnWeightRAG)\n",
    "# model.retrieve(\"What is NVIDIA\", topk=5)\n",
    "\n",
    "def generate_dragin_output(question):\n",
    "    output, contexts = model.inference(question, {}, f\"Question: {question}\\nAnswer:\")\n",
    "    return output, contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine similarity function\n",
    "def calculate_cosine_similarity(text1, text2):\n",
    "    if not text1 or not text2:  # Handle empty input\n",
    "        return 0.0\n",
    "    vectorizer = CountVectorizer().fit_transform([text1, text2])\n",
    "    vectors = vectorizer.toarray()\n",
    "    return cosine_similarity(vectors)[0, 1]\n",
    "\n",
    "# Text normalization function\n",
    "def normalize_answer(s):\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "# F1 Score calculation function\n",
    "def f1_score(prediction, ground_truth):\n",
    "    normalized_prediction = normalize_answer(prediction)\n",
    "    normalized_ground_truth = normalize_answer(ground_truth)\n",
    "\n",
    "    ZERO_METRIC = (0, 0, 0, 0.0)  # Include a default value for cosine similarity\n",
    "\n",
    "    # Handle binary answers separately\n",
    "    if normalized_prediction in ['yes', 'no', 'noanswer'] or normalized_ground_truth in ['yes', 'no', 'noanswer']:\n",
    "        if normalized_prediction != normalized_ground_truth:\n",
    "            return ZERO_METRIC\n",
    "\n",
    "    prediction_tokens = normalized_prediction.split()\n",
    "    ground_truth_tokens = normalized_ground_truth.split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "\n",
    "    if num_same == 0:\n",
    "        return ZERO_METRIC\n",
    "\n",
    "    precision = num_same / len(prediction_tokens)\n",
    "    recall = num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    cosine_sim = calculate_cosine_similarity(prediction, ground_truth)\n",
    "\n",
    "    return f1, precision, recall, cosine_sim\n",
    "\n",
    "\n",
    "# Load JSON data\n",
    "def load_json(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tqdm\n",
    "\n",
    "ground_truth_path = '<your dataset path>'\n",
    "\n",
    "with open(\"config.json\", 'r') as f:\n",
    "    config = json.load(f)\n",
    "gtp = ground_truth_path.replace('/', '_').replace('-', '_')\n",
    "mnp = config[\"model_name_or_path\"].replace('/', '_').replace('-', '_')\n",
    "answers_save_path = f'results/{mnp}_{gtp}_answers.json'\n",
    "all_contexts_save_path = f'results/{mnp}_{gtp}_contexts.json'\n",
    "with open(ground_truth_path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "answers = []\n",
    "all_contexts = []\n",
    "for d in tqdm.tqdm(data):\n",
    "    response, contexts = generate_dragin_output(d['question'])\n",
    "    answers.append({\n",
    "        'question': d['question'],\n",
    "        'answer': response\n",
    "    })\n",
    "    all_contexts.append({\n",
    "        'question': d['question'],\n",
    "        'contexts': str(contexts)\n",
    "    })\n",
    "with open(answers_save_path, 'w') as f:\n",
    "    json.dump(answers, f)\n",
    "with open(all_contexts_save_path, 'w') as f:\n",
    "    json.dump(all_contexts, f)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = answers_save_path\n",
    "llm=Groq\n",
    "\n",
    "# Load data\n",
    "question_chunks_data = load_json(json_file_path)\n",
    "ground_truth_data = load_json(ground_truth_path)\n",
    "\n",
    "# Map questions to ground truth answers\n",
    "ground_truth_map = {entry['question']: entry['answer'] for entry in ground_truth_data}\n",
    "\n",
    "# Initialize metrics\n",
    "total_f1, total_precision, total_recall, total_cos, count = 0, 0, 0, 0, 0\n",
    "\n",
    "# Evaluate F1, precision, recall, cosine similarity\n",
    "for question_entry in question_chunks_data:\n",
    "    question = question_entry.get('question', \"\")\n",
    "    ground_truth = ground_truth_map.get(question, \"\")\n",
    "    chunk1_text = question_entry.get('answer', \"\")\n",
    "\n",
    "    if chunk1_text:\n",
    "        f1, precision, recall, cos = f1_score(chunk1_text, ground_truth)\n",
    "        total_f1 += f1\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_cos += cos\n",
    "        count += 1\n",
    "\n",
    "avg_f1 = total_f1 / count if count else 0\n",
    "avg_precision = total_precision / count if count else 0\n",
    "avg_recall = total_recall / count if count else 0\n",
    "avg_cosine_sim = total_cos / count if count else 0\n",
    "\n",
    "# Print results\n",
    "print(f\"Average F1 Score: {avg_f1:.4f}\")\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average Cosine Similarity: {avg_cosine_sim:.4f}\")\n",
    "\n",
    "# Evaluate with LLM\n",
    "LLM_Score = 0\n",
    "for question_entry in question_chunks_data:\n",
    "    query = question_entry.get('question', \"\")\n",
    "    reply = question_entry.get('answer', \"\")\n",
    "    answer = ground_truth_map.get(query, \"\")\n",
    "\n",
    "    if query and reply and answer:\n",
    "        chat_completion = llm.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"\"\"Evaluate the semantic similarity between the following two answers with respect to some question. Output only a single floating-point number between 0 and 1, where 0 indicates no similarity and 1 indicates identical meaning. Respond with only the number:\n",
    "Question: {query}\n",
    "Text A: {answer}\n",
    "Text B: {reply}\"\"\"\n",
    "                }\n",
    "            ],\n",
    "            model=\"llama-3.1-70b-versatile\",\n",
    "            stream=False,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            score = float(chat_completion.choices[0].message.content.strip())\n",
    "            LLM_Score += score\n",
    "        except ValueError:\n",
    "            print(\"Invalid LLM response for query:\", query)\n",
    "\n",
    "# Calculate and print average LLM similarity score\n",
    "average_llm_score = LLM_Score / count if count else 0\n",
    "print(f\"Average LLM Similarity Score: {average_llm_score:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
