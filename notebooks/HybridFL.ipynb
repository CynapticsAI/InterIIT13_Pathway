{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook comprises of Multi-Agentic Domain Specific Pipeline with Pathway-based Hybrid RAG."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Necessary Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/openai/swarm.git\n",
    "!pip install networkx\n",
    "!pip install pandas\n",
    "!pip install torch \n",
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import networkx as nx\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import subprocess\n",
    "import urllib.parse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n",
    "from groq import Groq\n",
    "from swarm import Agent, Swarm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Necessary API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GROQ_API_KEY'] = \"YOUR-GROQ-API-KEY\"\n",
    "os.environ['OPENAI_API_KEY']='YOUR-OPENAI-API-KEY'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pathway-based Hybrid Retrieval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loads a prebuilt knowledge graph from a JSON file, reconstructing node attributes and edge weights.\n",
    "def load_graph_from_json(fname):\n",
    "    G = nx.Graph()\n",
    "\n",
    "    with open(fname, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    for node_data in data['nodes']:\n",
    "        node_name = node_data['name']\n",
    "        attributes = {\n",
    "            k: np.array(v) if isinstance(v, list) else v\n",
    "            for k, v in node_data['attributes'].items()\n",
    "        }\n",
    "        G.add_node(node_name, **attributes)\n",
    "\n",
    "    for edge_data in data['edges']:\n",
    "        G.add_edge(edge_data['source'], edge_data['target'], weight=edge_data['weight'])\n",
    "\n",
    "    return G\n",
    "\n",
    "# Performs graph rag search and returns top similar clusters \n",
    "def graph_rag_search(query, knowledge_graph, max_depth=1, min_similarity=0.4, max_words=500):\n",
    "    def compute_similarity(query_embedding, node_embedding):\n",
    "        return cosine_similarity([query_embedding], [node_embedding])[0][0]\n",
    "\n",
    "    def find_top_nodes(query_embedding, top_k=3):\n",
    "        similarity_scores = {\n",
    "            node: compute_similarity(query_embedding, data[\"embedding\"])\n",
    "            for node, data in knowledge_graph.nodes(data=True)\n",
    "        }\n",
    "        return sorted(\n",
    "            [(node, similarity) for node, similarity in similarity_scores.items() if similarity >= min_similarity],\n",
    "            key=lambda x: -x[1],\n",
    "        )[:top_k]\n",
    "\n",
    "    def expand_clusters(top_nodes):\n",
    "        clusters = []\n",
    "        for base_node, base_similarity in top_nodes:\n",
    "            cluster_nodes = []\n",
    "            neighbors = nx.single_source_shortest_path_length(\n",
    "                knowledge_graph, source=base_node, cutoff=max_depth\n",
    "            )\n",
    "            for neighbor, depth in neighbors.items():\n",
    "                similarity = compute_similarity(query_embedding, knowledge_graph.nodes[neighbor][\"embedding\"])\n",
    "                if similarity >= min_similarity:\n",
    "                    cluster_nodes.append((neighbor, similarity, depth))\n",
    "            clusters.append({\n",
    "                \"base_node\": base_node,\n",
    "                \"base_similarity\": base_similarity,\n",
    "                \"nodes\": cluster_nodes,\n",
    "            })\n",
    "        return clusters\n",
    "\n",
    "    def calculate_cluster_length(cluster):\n",
    "        return sum(\n",
    "            len(knowledge_graph.nodes[node][\"text\"].split())\n",
    "            for node, _, _ in cluster[\"nodes\"]\n",
    "        )\n",
    "\n",
    "    def build_combined_context(clusters):\n",
    "        context_segments = []\n",
    "        word_count = 0\n",
    "\n",
    "        for cluster in clusters:\n",
    "            # Concatenate all text in the current cluster\n",
    "            cluster_text = \" \".join(\n",
    "                knowledge_graph.nodes[node][\"text\"]\n",
    "                for node, _, _ in cluster[\"nodes\"]\n",
    "            )\n",
    "            cluster_words = len(cluster_text.split())\n",
    "\n",
    "            # Add the entire cluster if it fits within the max_words limit\n",
    "            if word_count + cluster_words <= max_words:\n",
    "                context_segments.append(cluster_text)\n",
    "                word_count += cluster_words\n",
    "            else:\n",
    "                break  # Stop adding clusters once the word limit is reached\n",
    "\n",
    "        return \" \".join(context_segments)\n",
    "\n",
    "    # Main Execution\n",
    "    query_embedding = embedding_model.encode([query])[0]\n",
    "\n",
    "    # Step 1: Find top 2-3 most similar nodes\n",
    "    top_nodes = find_top_nodes(query_embedding, top_k=3)\n",
    "    if not top_nodes:\n",
    "        return \"No relevant nodes found.\"\n",
    "\n",
    "    # Step 2: Expand clusters around the top nodes\n",
    "    clusters = expand_clusters(top_nodes)\n",
    "\n",
    "    # Step 3: Calculate total text length across clusters\n",
    "    total_text_length = sum(calculate_cluster_length(cluster) for cluster in clusters)\n",
    "\n",
    "    # Step 4: Build context\n",
    "    if total_text_length < max_words:\n",
    "        # Combine all clusters if the total text is less than max_words\n",
    "        context = build_combined_context(clusters)\n",
    "    else:\n",
    "        # Prioritize top clusters based on base similarity and combine them\n",
    "        clusters = sorted(clusters, key=lambda x: -x[\"base_similarity\"])\n",
    "        context = build_combined_context(clusters)\n",
    "    return context if context else \"No relevant content found.\"\n",
    "\n",
    "# Performs nodal similarity search to return most similar nodes\n",
    "def nodal_similarity_search(query, knowledge_graph, min_similarity=0.5, max_results=6):\n",
    "    query_embedding = embedding_model.encode([query])[0]\n",
    "\n",
    "    best_score, best_node = 0, None\n",
    "    relevant_nodes = []\n",
    "\n",
    "    for node in knowledge_graph.nodes:\n",
    "        chunk_text = knowledge_graph.nodes[node][\"text\"]\n",
    "        if not chunk_text:\n",
    "            continue\n",
    "\n",
    "        chunk_embedding = knowledge_graph.nodes[node][\"embedding\"]\n",
    "        content_similarity = cosine_similarity([query_embedding], [chunk_embedding])[0][0]\n",
    "\n",
    "        if content_similarity > min_similarity:\n",
    "            relevant_nodes.append((node, content_similarity))\n",
    "            if content_similarity > best_score:\n",
    "                best_score, best_node = content_similarity, node\n",
    "\n",
    "    relevant_nodes = sorted(relevant_nodes, key=lambda x: x[1], reverse=True)[:max_results]\n",
    "\n",
    "    if not relevant_nodes:\n",
    "        return \"No relevant content found. Try adjusting the query or similarity threshold.\"\n",
    "\n",
    "    context_segments = [knowledge_graph.nodes[node][\"text\"] for node, _ in relevant_nodes]\n",
    "    context = \" \".join(context_segments)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg = load_graph_from_json(r\"knowledge_graph\\financekg.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main retrieval function to execute Pathway and custom retrieval functions\n",
    "def execute_curl_request(query, k=1):\n",
    "    \"\"\"\n",
    "    description: This function retrieves relevant information from a database consisting of financial information\"\"\"\n",
    "    # URL encode the query\n",
    "    encoded_query = urllib.parse.quote(query)\n",
    "    url = f\"http://localhost:8000/v1/retrieve?query={encoded_query}&k={k}\"\n",
    "    \n",
    "    # Construct the curl command\n",
    "    command = [\n",
    "        \"curl\", \"-X\", \"GET\", url, \"-H\", \"accept: */*\"\n",
    "    ]\n",
    "    \n",
    "    # Execute the command\n",
    "    try:\n",
    "        result = subprocess.run(command, capture_output=True, text=True, check=True)\n",
    "        response = result.stdout\n",
    "        \n",
    "        # Parse the JSON response\n",
    "        data = json.loads(response)\n",
    "        \n",
    "        # Extract and print only the \"text\" field\n",
    "        for item in data:\n",
    "            ptext=item.get(\"text\")\n",
    "            # print(\"Text:\\n\", item.get(\"text\"))\n",
    "            i=1\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Error:\", e.stderr)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Failed to parse JSON response.\")\n",
    "    s=\"Text:\\n\"+item.get(\"text\")+\"done\"\n",
    "    x = graph_rag_search(query,kg)\n",
    "    y = nodal_similarity_search(query,kg)\n",
    "    return y+s+x\n",
    "\n",
    "def execute_curl_request_legal(query, k=3):\n",
    "    \"\"\"\n",
    "    description: This function retrieves relevant information from a database consisting of financial information\"\"\"\n",
    "    # URL encode the query\n",
    "    encoded_query = urllib.parse.quote(query)\n",
    "    url = f\"http://localhost:8001/v1/retrieve?query={encoded_query}&k={k}\"\n",
    "    \n",
    "    # Construct the curl command\n",
    "    command = [\n",
    "        \"curl\", \"-X\", \"GET\", url, \"-H\", \"accept: */*\"\n",
    "    ]\n",
    "    \n",
    "    # Execute the command\n",
    "    try:\n",
    "        result = subprocess.run(command, capture_output=True, text=True, check=True)\n",
    "        response = result.stdout\n",
    "        \n",
    "        # Parse the JSON response\n",
    "        data = json.loads(response)\n",
    "        \n",
    "        # Extract and print only the \"text\" field\n",
    "        for item in data:\n",
    "            ptext=item.get(\"text\")\n",
    "            # print(\"Text:\\n\", item.get(\"text\"))\n",
    "            i=1\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Error:\", e.stderr)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Failed to parse JSON response.\")\n",
    "    s=\"Text:\\n\"+item.get(\"text\")+\"done\"\n",
    "    return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Agentic Generative Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_v=\"gpt-4o-mini\"\n",
    "client = Swarm()\n",
    "\n",
    "def transfer_to_agent_b():\n",
    "    return agent_b\n",
    "def transfer_to_agent_c():\n",
    "    return agent_c\n",
    "def transfer_to_agent_a():\n",
    "    return agent_a\n",
    "\n",
    "agent_a = Agent(\n",
    "    name=\"query simplifier\",\n",
    "    model=agent_v,\n",
    "    instructions= \"\"\"You are a query simplification agent. A question answerer agent can ask you to simplify a complex MultiHop query into its constituent simplified queries and return the subqueries to Task planning agent or the question answerer agent. Follow these rules:\n",
    "\n",
    "    Simplify only when the input query contains multiple sub-questions. If there is only one question, do not modify it.\n",
    "    Preserve meaning completely. The simplified sub-queries must collectively capture the full meaning and intent of the original query.\n",
    "    Provide only the simplified sub-queries as output. Do not include explanations, comments, or formatting beyond the sub-queries themselves.\n",
    "    Examples\n",
    "    Example 1:\n",
    "    Input:\n",
    "    What strategic goals do Apple, Johnson & Johnson, and NVIDIA aim to achieve through their R&D investments, and how do these goals support their competitive position?\n",
    "\n",
    "    Output:\n",
    "\n",
    "    What are Apple’s strategic R&D goals?\n",
    "    What are Johnson & Johnson’s strategic R&D goals?\n",
    "    What are NVIDIA’s strategic R&D goals?\n",
    "    How does Apple’s R&D strategy support its competitive position?\n",
    "    How does Johnson & Johnson’s R&D strategy support its competitive position?\n",
    "    How does NVIDIA’s R&D strategy support its competitive position?\n",
    "    Example 2:\n",
    "    Input:\n",
    "    What are NVIDIA’s strategies for addressing the gaming and data center markets, and how do they utilize their GPU architecture in these areas?\n",
    "\n",
    "    Output:\n",
    "\n",
    "    What are NVIDIA’s strategies for addressing the gaming market?\n",
    "    What are NVIDIA’s strategies for addressing the data center market?\n",
    "    How does NVIDIA leverage GPU architecture for gaming?\n",
    "    How does NVIDIA leverage GPU architecture for data centers?\n",
    "\n",
    "    Adhere strictly to the above rules and examples in your responses.\n",
    "                                          \"\"\",\n",
    "    functions=[transfer_to_agent_b,transfer_to_agent_c],\n",
    ")\n",
    "\n",
    "agent_b = Agent(\n",
    "    name=\"task planner\",\n",
    "    model=agent_v,\n",
    "    instructions=\"\"\"You are a Task Planning Agent within a retrieval-based pipeline. Your role is to create an optimal sequence of actions for answering complex queries and return these ordered subqueries to the question answering agent to finally answer the question.\n",
    "\n",
    "    Responsibilities:\n",
    "    Input: You will receive a complex query along with its simplified sub-queries.\n",
    "    Output: Your task is to order the sub-queries logically to ensure the original query is answered comprehensively and efficiently. After that, you are mandatorily needed to return the ordered subqueries to question aswerer.\n",
    "    You will be penalized if you donot return the ordered subqueries to question answerer.\n",
    "    Rules for Task Planning:\n",
    "    Dependency First: If answering one sub-query is essential for addressing another, the dependent sub-query must appear later in the sequence.\n",
    "    Logical Flow: Arrange sub-queries to reflect a natural progression of information, building context where necessary.\n",
    "    Completeness: The ordered sub-queries must collectively address the original query.\n",
    "    No Extra Content: Your response must include only the ordered list of sub-queries, without additional comments or explanations.\n",
    "    Examples\n",
    "    Example 1:\n",
    "    Input:\n",
    "    Query:\n",
    "    What is the income in the last five years of the company whose income in the year 2022 was the second highest?\n",
    "\n",
    "    Sub-Queries (Unordered):\n",
    "\n",
    "    What is the income in the last five years of the company?\n",
    "    Which company had the second highest income in the year 2022?\n",
    "\n",
    "    Output:\n",
    "    Ordered Sub-Queries:\n",
    "    Which company had the second highest income in the year 2022?\n",
    "    What is the income in the last five years of the company?\n",
    "    \n",
    "    Adhere strictly to the above rules and examples in your responses.\n",
    "                                   \"\"\",\n",
    "    functions=[transfer_to_agent_c],\n",
    ")\n",
    "agent_c = Agent(\n",
    "    name=\"question answerer\",\n",
    "    model=agent_v,\n",
    "    instructions=\"\"\" You are Question answering agent,\n",
    "    you may be given multihop questions i.e a question containing multiple subqueries or single hop query\n",
    "    you can use any of the tools at your disposal to answer this query accurately.\n",
    "\n",
    "    if the query is multihop you should use the query simplifier agent to get the simplified subqueries and then use the task planner agent to order them correctly.\n",
    "    if the query is single hop you can directly use the execute_curl_request function to get the context.\n",
    "\n",
    "    Tools and agents:\n",
    "    query simplifier agents: If and only If you decide that the question given is multihop you should use this query simplifier agent which gives you the simplified subqueries. \n",
    "    Task planner agent: when you have the simplified subqueries you should give them to Task Planner to order them correctly.\n",
    "    execute_curl_request: This function retrieves relevant information from a database consisting of financial information, use this when your existing knowledge base is not sufficient to answer questions of financial questions about companies\n",
    "\n",
    "    call the retriever function with appropriate queries to recieve usefull contexts. If the context you recieve useless rephrase the query to get appropriate context.\n",
    "\n",
    "    Your final output should be the answer only and it should be based off the context retrieved by the helper functions. In case the context is not helpful, \n",
    "    answer from your own knowledge base.\n",
    "    \n",
    "    example 1:\n",
    "    Query: What were the operating leases for the years 2023 and 2022 for the company where up to 47 million shares of common stock could be used as stock awards? Also, describe the 2007 Plan which assured these stock awards.\n",
    "\n",
    "    Thought process: \n",
    "    This is a multihop query, so we need to simplify it into subqueries\n",
    "    Handover query to query simplifier to get subqueries\n",
    "    Give the sub queries to Task Planner to order them\n",
    "    retrieve the sub queries context from execute_curl request\n",
    "    Answer the main question using the context.\n",
    "\n",
    "    Answer: For the fiscal years 2023 and 2022, the company's operating lease expenses were $193 million and $168 million, respectively. The company referred to is NVIDIA Corporation, which has up to 47 million shares of common stock available to be issued as stock awards under the 2007 Equity Incentive Plan. The 2007 Plan, approved by shareholders in 2007 and most recently amended and restated, authorizes the issuance of various stock-based awards to employees, directors, and consultants. These awards include incentive stock options, non-statutory stock options, restricted stock, restricted stock units (RSUs), stock appreciation rights, performance stock awards, performance cash awards, and other stock-based awards. As of January 29, 2023, up to 47 million shares could be issued pursuant to stock awards granted under this plan.\n",
    "\n",
    "    example 2:\n",
    "\n",
    "    Query: What is the name of the registrant as specified in its charter that had a Form 10-K filed for the fiscal year ended January 29, 2023\n",
    "\n",
    "    Thought process:\n",
    "    This is a single hop query, \n",
    "    So we can directly use the execute_curl_request function to get the context\n",
    "    Answer the question using the context.\n",
    "\n",
    "    Answer: The name of the registrant is NVIDIA CORPORATION.\\n\\nOR   ALSO CAN BE\\n\\n [[User]] What is the Name of Nasdaq symbol And Which Market is the share registered on  for an Exchange under the 10K filing done by The  nvidia corporation\n",
    "    \"\"\",\n",
    "    functions=[transfer_to_agent_a, transfer_to_agent_b, execute_curl_request],\n",
    ")\n",
    "agent_d = Agent(\n",
    "    name=\"evaluator agent\",\n",
    "    model=agent_v,\n",
    "    instructions= \"\"\" You are an answer evaluator agent, you will be given a question and its corrosponding answer, if the answer satisfies the question output the same answer, without the question.\n",
    "    If it does not satisfy the query completely then you need to call agent: question answerer and give it the original query. the answer should satisfy the entire question. it should not mention that the retrieved context was not helpful\n",
    "    if any such mention is present in the answer the recall agent question answerer and give it the question. be strict. \n",
    "    \"\"\",\n",
    "    functions=[transfer_to_agent_c ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qna(iquery):\n",
    "    iquery = (\n",
    "        \"I am going to give you a multihop question or a single hop question, a question containing multiple sub queries,, determine this and do the appropriate process \\n IF THE QUESTION IS MULTIHOP- Your process should be to break down the question into sub queries, order them, retrieve required context using subqueries, and give one cohesive answer. \\n IF THE QUESTION IS SINGLE HOP- Directly execute curl request and answer the query \\n return to me only the answer and nothing else. \\n Question: \"\n",
    "        + iquery\n",
    "    )\n",
    "    response = client.run(\n",
    "        agent=agent_c,\n",
    "        messages=[{\"role\": \"user\", \"content\": iquery}],\n",
    "        debug=True\n",
    "    )\n",
    "    op=response.messages[-1][\"content\"]\n",
    "\n",
    "    eval_query=\"I am going to give you a input query and the generated answer for it, if the answer satisfies the generated query then output the answer directly else give the query back to the agent \\n Query:\"+iquery+\"\\n Answer:\"+op\n",
    "\n",
    "    response = client.run(\n",
    "        agent=agent_d,\n",
    "        messages=[{\"role\": \"user\", \"content\": eval_query}],\n",
    "        max_turns=15,\n",
    "        debug=True\n",
    "    )\n",
    "\n",
    "    fop=response.messages[-1][\"content\"]\n",
    "    return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### evaluation functions ####\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from groq import Groq\n",
    "llm = Groq()\n",
    "\n",
    "# Cosine similarity function\n",
    "def calculate_cosine_similarity(text1, text2):\n",
    "    if not text1 or not text2:  # Handle empty input\n",
    "        return 0.0\n",
    "    vectorizer = CountVectorizer().fit_transform([text1, text2])\n",
    "    vectors = vectorizer.toarray()\n",
    "    return cosine_similarity(vectors)[0, 1]\n",
    "\n",
    "# Text normalization function\n",
    "def normalize_answer(s):\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "# F1 Score calculation function\n",
    "def f1_score(prediction, ground_truth):\n",
    "    normalized_prediction = normalize_answer(prediction)\n",
    "    normalized_ground_truth = normalize_answer(ground_truth)\n",
    "\n",
    "    ZERO_METRIC = (0, 0, 0, 0.0)  # Include a default value for cosine similarity\n",
    "\n",
    "    # Handle binary answers separately\n",
    "    if normalized_prediction in ['yes', 'no', 'noanswer'] or normalized_ground_truth in ['yes', 'no', 'noanswer']:\n",
    "        if normalized_prediction != normalized_ground_truth:\n",
    "            return ZERO_METRIC\n",
    "\n",
    "    prediction_tokens = normalized_prediction.split()\n",
    "    ground_truth_tokens = normalized_ground_truth.split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "\n",
    "    if num_same == 0:\n",
    "        return ZERO_METRIC\n",
    "\n",
    "    precision = num_same / len(prediction_tokens)\n",
    "    recall = num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    cosine_sim = calculate_cosine_similarity(prediction, ground_truth)\n",
    "\n",
    "    return f1, precision, recall, cosine_sim\n",
    "\n",
    "\n",
    "# Load JSON data\n",
    "def load_json(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINANCIAL INFERENCE AND EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MULTIHOP INFERENCE \n",
    "input_file = r\"datasets\\multihop_finance.json\"  \n",
    "output_file = r\"notebooks\\HybridFL_on_multihop.json\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "results = []\n",
    "\n",
    "for entry in data:\n",
    "    question = entry[\"question\"]\n",
    "    print(f\"Querying for question: {question}\")\n",
    "    try:\n",
    "        response = qna(question)\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": response\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying agent for question: {question}\\nError: {e}\")\n",
    "\n",
    "with open(output_file, \"w\") as file:\n",
    "    json.dump(results, file, indent=4)\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### MULTIHOP EVAL ####\n",
    "\n",
    "# Paths to the input files\n",
    "json_file_path = output_file\n",
    "ground_truth_path = input_file\n",
    "\n",
    "# Load data\n",
    "question_chunks_data = load_json(json_file_path)\n",
    "ground_truth_data = load_json(ground_truth_path)\n",
    "\n",
    "# Map questions to ground truth answers\n",
    "ground_truth_map = {entry['question']: entry['answer'] for entry in ground_truth_data}\n",
    "\n",
    "# Initialize metrics\n",
    "total_f1, total_precision, total_recall, total_cos, count = 0, 0, 0, 0, 0\n",
    "\n",
    "# Evaluate F1, precision, recall, cosine similarity\n",
    "for question_entry in question_chunks_data:\n",
    "    question = question_entry.get('question', \"\")\n",
    "    ground_truth = ground_truth_map.get(question, \"\")\n",
    "    chunk1_text = question_entry.get('answer', \"\")\n",
    "\n",
    "    if chunk1_text:  # Process only if answer exists\n",
    "        f1, precision, recall, cos = f1_score(chunk1_text, ground_truth)\n",
    "        total_f1 += f1\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_cos += cos\n",
    "        count += 1\n",
    "\n",
    "# Average metrics\n",
    "avg_f1 = total_f1 / count if count else 0\n",
    "avg_precision = total_precision / count if count else 0\n",
    "avg_recall = total_recall / count if count else 0\n",
    "avg_cosine_sim = total_cos / count if count else 0\n",
    "\n",
    "# Print results\n",
    "print(f\"Average F1 Score: {avg_f1:.4f}\")\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average Cosine Similarity: {avg_cosine_sim:.4f}\")\n",
    "\n",
    "# Evaluate with LLM\n",
    "LLM_Score = 0\n",
    "for question_entry in question_chunks_data:\n",
    "    query = question_entry.get('question', \"\")\n",
    "    reply = question_entry.get('answer', \"\")\n",
    "    answer = ground_truth_map.get(query, \"\")\n",
    "\n",
    "    if query and reply and answer:\n",
    "        chat_completion = llm.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"\"\"Evaluate the semantic similarity between the following two answers with respect to some question. Output only a single floating-point number between 0 and 1, where 0 indicates no similarity and 1 indicates identical meaning. Respond with only the number:\n",
    "Question: {query}\n",
    "Text A: {answer}\n",
    "Text B: {reply}\"\"\"\n",
    "                }\n",
    "            ],\n",
    "            model=\"llama-3.1-70b-versatile\",\n",
    "            stream=False,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            score = float(chat_completion.choices[0].message.content.strip())\n",
    "            LLM_Score += score\n",
    "        except ValueError:\n",
    "            print(\"Invalid LLM response for query:\", query)\n",
    "\n",
    "# Calculate and print average LLM similarity score\n",
    "average_llm_score = LLM_Score / count if count else 0\n",
    "print(f\"Average LLM Similarity Score: {average_llm_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SINGLEHOP FINANCE INFERENCE \n",
    "input_file = r\"datasets\\singlehop_finance.json\"  \n",
    "output_file = r\"notebooks\\HybridFL_on_singlehop_finance.json\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "results = []\n",
    "\n",
    "for entry in data:\n",
    "    question = entry[\"question\"]\n",
    "    print(f\"Querying for question: {question}\")\n",
    "    try:\n",
    "        response = qna(question)\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": response\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying agent for question: {question}\\nError: {e}\")\n",
    "\n",
    "with open(output_file, \"w\") as file:\n",
    "    json.dump(results, file, indent=4)\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SINGLEHOP EVAL ####\n",
    "\n",
    "# Paths to the input files\n",
    "json_file_path = output_file\n",
    "ground_truth_path = input_file\n",
    "\n",
    "# Load data\n",
    "question_chunks_data = load_json(json_file_path)\n",
    "ground_truth_data = load_json(ground_truth_path)\n",
    "\n",
    "# Map questions to ground truth answers\n",
    "ground_truth_map = {entry['question']: entry['answer'] for entry in ground_truth_data}\n",
    "\n",
    "# Initialize metrics\n",
    "total_f1, total_precision, total_recall, total_cos, count = 0, 0, 0, 0, 0\n",
    "\n",
    "# Evaluate F1, precision, recall, cosine similarity\n",
    "for question_entry in question_chunks_data:\n",
    "    question = question_entry.get('question', \"\")\n",
    "    ground_truth = ground_truth_map.get(question, \"\")\n",
    "    chunk1_text = question_entry.get('answer', \"\")\n",
    "\n",
    "    if chunk1_text:  # Process only if answer exists\n",
    "        f1, precision, recall, cos = f1_score(chunk1_text, ground_truth)\n",
    "        total_f1 += f1\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_cos += cos\n",
    "        count += 1\n",
    "\n",
    "# Average metrics\n",
    "avg_f1 = total_f1 / count if count else 0\n",
    "avg_precision = total_precision / count if count else 0\n",
    "avg_recall = total_recall / count if count else 0\n",
    "avg_cosine_sim = total_cos / count if count else 0\n",
    "\n",
    "# Print results\n",
    "print(f\"Average F1 Score: {avg_f1:.4f}\")\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average Cosine Similarity: {avg_cosine_sim:.4f}\")\n",
    "\n",
    "# Evaluate with LLM\n",
    "LLM_Score = 0\n",
    "for question_entry in question_chunks_data:\n",
    "    query = question_entry.get('question', \"\")\n",
    "    reply = question_entry.get('answer', \"\")\n",
    "    answer = ground_truth_map.get(query, \"\")\n",
    "\n",
    "    if query and reply and answer:\n",
    "        chat_completion = llm.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"\"\"Evaluate the semantic similarity between the following two answers with respect to some question. Output only a single floating-point number between 0 and 1, where 0 indicates no similarity and 1 indicates identical meaning. Respond with only the number:\n",
    "Question: {query}\n",
    "Text A: {answer}\n",
    "Text B: {reply}\"\"\"\n",
    "                }\n",
    "            ],\n",
    "            model=\"llama-3.1-70b-versatile\",\n",
    "            stream=False,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            score = float(chat_completion.choices[0].message.content.strip())\n",
    "            LLM_Score += score\n",
    "        except ValueError:\n",
    "            print(\"Invalid LLM response for query:\", query)\n",
    "\n",
    "# Calculate and print average LLM similarity score\n",
    "average_llm_score = LLM_Score / count if count else 0\n",
    "print(f\"Average LLM Similarity Score: {average_llm_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEGAL INFERENCE AND EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_v=\"gpt-4o-mini\"\n",
    "client = Swarm()\n",
    "\n",
    "def transfer_to_agent_b():\n",
    "    return agent_b\n",
    "def transfer_to_agent_c():\n",
    "    return agent_c\n",
    "def transfer_to_agent_a():\n",
    "    return agent_a\n",
    "\n",
    "agent_a = Agent(\n",
    "    name=\"query simplifier\",\n",
    "    model=agent_v,\n",
    "    instructions= \"\"\"You are a query simplification agent. A question answerer agent can ask you to simplify a complex MultiHop query into its constituent simplified queries and return the subqueries to Task planning agent or the question answerer agent. Follow these rules:\n",
    "\n",
    "    Simplify only when the input query contains multiple sub-questions. If there is only one question, do not modify it.\n",
    "    Preserve meaning completely. The simplified sub-queries must collectively capture the full meaning and intent of the original query.\n",
    "    Provide only the simplified sub-queries as output. Do not include explanations, comments, or formatting beyond the sub-queries themselves.\n",
    "    Examples\n",
    "    Example 1:\n",
    "    Input:\n",
    "    What strategic goals do Apple, Johnson & Johnson, and NVIDIA aim to achieve through their R&D investments, and how do these goals support their competitive position?\n",
    "\n",
    "    Output:\n",
    "\n",
    "    What are Apple’s strategic R&D goals?\n",
    "    What are Johnson & Johnson’s strategic R&D goals?\n",
    "    What are NVIDIA’s strategic R&D goals?\n",
    "    How does Apple’s R&D strategy support its competitive position?\n",
    "    How does Johnson & Johnson’s R&D strategy support its competitive position?\n",
    "    How does NVIDIA’s R&D strategy support its competitive position?\n",
    "    Example 2:\n",
    "    Input:\n",
    "    What are NVIDIA’s strategies for addressing the gaming and data center markets, and how do they utilize their GPU architecture in these areas?\n",
    "\n",
    "    Output:\n",
    "\n",
    "    What are NVIDIA’s strategies for addressing the gaming market?\n",
    "    What are NVIDIA’s strategies for addressing the data center market?\n",
    "    How does NVIDIA leverage GPU architecture for gaming?\n",
    "    How does NVIDIA leverage GPU architecture for data centers?\n",
    "\n",
    "    Adhere strictly to the above rules and examples in your responses.\n",
    "                                          \"\"\",\n",
    "    functions=[transfer_to_agent_b,transfer_to_agent_c],\n",
    ")\n",
    "\n",
    "agent_b = Agent(\n",
    "    name=\"task planner\",\n",
    "    model=agent_v,\n",
    "    instructions=\"\"\"You are a Task Planning Agent within a retrieval-based pipeline. Your role is to create an optimal sequence of actions for answering complex queries and return these ordered subqueries to the question answering agent to finally answer the question.\n",
    "\n",
    "    Responsibilities:\n",
    "    Input: You will receive a complex query along with its simplified sub-queries.\n",
    "    Output: Your task is to order the sub-queries logically to ensure the original query is answered comprehensively and efficiently. After that, you are mandatorily needed to return the ordered subqueries to question aswerer.\n",
    "    You will be penalized if you donot return the ordered subqueries to question answerer.\n",
    "    Rules for Task Planning:\n",
    "    Dependency First: If answering one sub-query is essential for addressing another, the dependent sub-query must appear later in the sequence.\n",
    "    Logical Flow: Arrange sub-queries to reflect a natural progression of information, building context where necessary.\n",
    "    Completeness: The ordered sub-queries must collectively address the original query.\n",
    "    No Extra Content: Your response must include only the ordered list of sub-queries, without additional comments or explanations.\n",
    "    Examples\n",
    "    Example 1:\n",
    "    Input:\n",
    "    Query:\n",
    "    What is the income in the last five years of the company whose income in the year 2022 was the second highest?\n",
    "\n",
    "    Sub-Queries (Unordered):\n",
    "\n",
    "    What is the income in the last five years of the company?\n",
    "    Which company had the second highest income in the year 2022?\n",
    "\n",
    "    Output:\n",
    "    Ordered Sub-Queries:\n",
    "    Which company had the second highest income in the year 2022?\n",
    "    What is the income in the last five years of the company?\n",
    "    \n",
    "    Adhere strictly to the above rules and examples in your responses.\n",
    "                                   \"\"\",\n",
    "    functions=[transfer_to_agent_c],\n",
    ")\n",
    "agent_c = Agent(\n",
    "    name=\"question answerer\",\n",
    "    model=agent_v,\n",
    "    instructions=\"\"\" You are Question answering agent,\n",
    "    you may be given multihop questions i.e a question containing multiple subqueries or single hop query\n",
    "    you can use any of the tools at your disposal to answer this query accurately.\n",
    "\n",
    "    if the query is multihop you should use the query simplifier agent to get the simplified subqueries and then use the task planner agent to order them correctly.\n",
    "    if the query is single hop you can directly use the execute_curl_request function to get the context.\n",
    "\n",
    "    Tools and agents:\n",
    "    query simplifier agents: If and only If you decide that the question given is multihop you should use this query simplifier agent which gives you the simplified subqueries. \n",
    "    Task planner agent: when you have the simplified subqueries you should give them to Task Planner to order them correctly.\n",
    "    execute_curl_request: This function retrieves relevant information from a database consisting of financial information, use this when your existing knowledge base is not sufficient to answer questions of financial questions about companies\n",
    "\n",
    "    call the retriever function with appropriate queries to recieve usefull contexts. If the context you recieve useless rephrase the query to get appropriate context.\n",
    "\n",
    "    Your final output should be the answer only and it should be based off the context retrieved by the helper functions. In case the context is not helpful, \n",
    "    answer from your own knowledge base.\n",
    "    \n",
    "    example 1:\n",
    "    Query: What were the operating leases for the years 2023 and 2022 for the company where up to 47 million shares of common stock could be used as stock awards? Also, describe the 2007 Plan which assured these stock awards.\n",
    "\n",
    "    Thought process: \n",
    "    This is a multihop query, so we need to simplify it into subqueries\n",
    "    Handover query to query simplifier to get subqueries\n",
    "    Give the sub queries to Task Planner to order them\n",
    "    retrieve the sub queries context from execute_curl request\n",
    "    Answer the main question using the context.\n",
    "\n",
    "    Answer: For the fiscal years 2023 and 2022, the company's operating lease expenses were $193 million and $168 million, respectively. The company referred to is NVIDIA Corporation, which has up to 47 million shares of common stock available to be issued as stock awards under the 2007 Equity Incentive Plan. The 2007 Plan, approved by shareholders in 2007 and most recently amended and restated, authorizes the issuance of various stock-based awards to employees, directors, and consultants. These awards include incentive stock options, non-statutory stock options, restricted stock, restricted stock units (RSUs), stock appreciation rights, performance stock awards, performance cash awards, and other stock-based awards. As of January 29, 2023, up to 47 million shares could be issued pursuant to stock awards granted under this plan.\n",
    "\n",
    "    example 2:\n",
    "\n",
    "    Query: What is the name of the registrant as specified in its charter that had a Form 10-K filed for the fiscal year ended January 29, 2023\n",
    "\n",
    "    Thought process:\n",
    "    This is a single hop query, \n",
    "    So we can directly use the execute_curl_request function to get the context\n",
    "    Answer the question using the context.\n",
    "\n",
    "    Answer: The name of the registrant is NVIDIA CORPORATION.\\n\\nOR   ALSO CAN BE\\n\\n [[User]] What is the Name of Nasdaq symbol And Which Market is the share registered on  for an Exchange under the 10K filing done by The  nvidia corporation\n",
    "    \"\"\",\n",
    "    functions=[transfer_to_agent_a, transfer_to_agent_b, execute_curl_request_legal],\n",
    ")\n",
    "agent_d = Agent(\n",
    "    name=\"evaluator agent\",\n",
    "    model=agent_v,\n",
    "    instructions= \"\"\" You are an answer evaluator agent, you will be given a question and its corrosponding answer, if the answer satisfies the question output the same answer, without the question.\n",
    "    If it does not satisfy the query completely then you need to call agent: question answerer and give it the original query. the answer should satisfy the entire question. it should not mention that the retrieved context was not helpful\n",
    "    if any such mention is present in the answer the recall agent question answerer and give it the question. be strict. \n",
    "    \"\"\",\n",
    "    functions=[transfer_to_agent_c ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg = load_graph_from_json(r\"knowledge_graph\\cuadkg.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CUAD INFERENCE ### \n",
    "input_file = r\"datasets\\CUAD_LBRAG.json\"  \n",
    "output_file = r\"notebooks\\HybridFL_on_CUAD_LBRAG.json\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "results = []\n",
    "\n",
    "for entry in data:\n",
    "    question = entry[\"question\"]\n",
    "    print(f\"Querying for question: {question}\")\n",
    "    try:\n",
    "        response = qna(question)\n",
    "        results.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": response\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying agent for question: {question}\\nError: {e}\")\n",
    "\n",
    "with open(output_file, \"w\") as file:\n",
    "    json.dump(results, file, indent=4)\n",
    "print(f\"Results saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CUAD EVAL ####\n",
    "\n",
    "# Paths to the input files\n",
    "json_file_path = output_file\n",
    "ground_truth_path = input_file\n",
    "\n",
    "# Load data\n",
    "question_chunks_data = load_json(json_file_path)\n",
    "ground_truth_data = load_json(ground_truth_path)\n",
    "\n",
    "# Map questions to ground truth answers\n",
    "ground_truth_map = {entry['question']: entry['answer'] for entry in ground_truth_data}\n",
    "\n",
    "# Initialize metrics\n",
    "total_f1, total_precision, total_recall, total_cos, count = 0, 0, 0, 0, 0\n",
    "\n",
    "# Evaluate F1, precision, recall, cosine similarity\n",
    "for question_entry in question_chunks_data:\n",
    "    question = question_entry.get('question', \"\")\n",
    "    ground_truth = ground_truth_map.get(question, \"\")\n",
    "    chunk1_text = question_entry.get('answer', \"\")\n",
    "\n",
    "    if chunk1_text:  # Process only if answer exists\n",
    "        f1, precision, recall, cos = f1_score(chunk1_text, ground_truth)\n",
    "        total_f1 += f1\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_cos += cos\n",
    "        count += 1\n",
    "\n",
    "# Average metrics\n",
    "avg_f1 = total_f1 / count if count else 0\n",
    "avg_precision = total_precision / count if count else 0\n",
    "avg_recall = total_recall / count if count else 0\n",
    "avg_cosine_sim = total_cos / count if count else 0\n",
    "\n",
    "# Print results\n",
    "print(f\"Average F1 Score: {avg_f1:.4f}\")\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average Cosine Similarity: {avg_cosine_sim:.4f}\")\n",
    "\n",
    "# Evaluate with LLM\n",
    "LLM_Score = 0\n",
    "for question_entry in question_chunks_data:\n",
    "    query = question_entry.get('question', \"\")\n",
    "    reply = question_entry.get('answer', \"\")\n",
    "    answer = ground_truth_map.get(query, \"\")\n",
    "\n",
    "    if query and reply and answer:\n",
    "        chat_completion = llm.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"\"\"Evaluate the semantic similarity between the following two answers with respect to some question. Output only a single floating-point number between 0 and 1, where 0 indicates no similarity and 1 indicates identical meaning. Respond with only the number:\n",
    "Question: {query}\n",
    "Text A: {answer}\n",
    "Text B: {reply}\"\"\"\n",
    "                }\n",
    "            ],\n",
    "            model=\"llama-3.1-70b-versatile\",\n",
    "            stream=False,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            score = float(chat_completion.choices[0].message.content.strip())\n",
    "            LLM_Score += score\n",
    "        except ValueError:\n",
    "            print(\"Invalid LLM response for query:\", query)\n",
    "\n",
    "# Calculate and print average LLM similarity score\n",
    "average_llm_score = LLM_Score / count if count else 0\n",
    "print(f\"Average LLM Similarity Score: {average_llm_score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
