{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U langchain-openai -qqq\n",
    "!pip install langchain\n",
    "!pip install langchain-community\n",
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports \n",
    "from __future__ import annotations\n",
    "import subprocess\n",
    "import urllib.parse\n",
    "import os\n",
    "import json\n",
    "import openai\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_community.vectorstores import PathwayVectorClient\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "from typing import Any, Dict, List, Optional, Sequence, Tuple\n",
    "import numpy as np\n",
    "from langchain_core.callbacks import (\n",
    "    CallbackManagerForChainRun,\n",
    ")\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain_core.messages import AIMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import BasePromptTemplate\n",
    "from langchain_core.runnables import Runnable\n",
    "from pydantic import Field\n",
    "\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.chains.flare.prompts import (\n",
    "    PROMPT,\n",
    "    QUESTION_GENERATOR_PROMPT,\n",
    "    FinishedOutputParser,\n",
    ")\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain_core.vectorstores import VectorStore\n",
    "import time\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "import json\n",
    "import PyPDF2\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from groq import Groq\n",
    "# Load the benchmarking queries\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Necessary API-KEYs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global memory_dict\n",
    "memory_dict = {}\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"Your OpenAI API Key\"\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", logprobs=True)\n",
    "os.environ['GROQ_API_KEY'] = \"Your groq API Key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-up Pathway Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_curl_request(query, k=2):\n",
    "    # URL encode the query\n",
    "    encoded_query = urllib.parse.quote(query)\n",
    "    url = f\"http://localhost:8000/v1/retrieve?query={encoded_query}&k={k}\"\n",
    "    \n",
    "    # Construct the curl command\n",
    "    command = [\n",
    "        \"curl\", \"-X\", \"GET\", url, \"-H\", \"accept: */*\"\n",
    "    ]\n",
    "    \n",
    "    # Execute the command\n",
    "    try:\n",
    "        result = subprocess.run(command, capture_output=True, text=True, check=True)\n",
    "        print(\"Response:\\n\", result.stdout)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Error:\", e.stderr)\n",
    "\n",
    "client = PathwayVectorClient(url=\"http://localhost:8000\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Agentic Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_tokens_and_log_probs(response: AIMessage) -> Tuple[List[str], List[float]]:\n",
    "    \"\"\"Extract tokens and log probabilities from chat model response.\"\"\"\n",
    "    tokens = []\n",
    "    log_probs = []\n",
    "    print(f\"####### Model_Used: {response.response_metadata['model_name']} ########\")\n",
    "    for token in response.response_metadata[\"logprobs\"][\"content\"]:\n",
    "        tokens.append(token[\"token\"])\n",
    "        log_probs.append(token[\"logprob\"])\n",
    "    # if response:\n",
    "    #   print(f\"####### Model_Used: {response.model_name} ########\")\n",
    "    return tokens, log_probs\n",
    "\n",
    "\n",
    "class QuestionGeneratorChain(LLMChain):\n",
    "    \"\"\"Chain that generates questions from uncertain spans.\"\"\"\n",
    "\n",
    "    prompt: BasePromptTemplate = QUESTION_GENERATOR_PROMPT\n",
    "    \"\"\"Prompt template for the chain.\"\"\"\n",
    "\n",
    "    @classmethod\n",
    "    def is_lc_serializable(cls) -> bool:\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"Input keys for the chain.\"\"\"\n",
    "        return [\"user_input\", \"context\", \"response\"]\n",
    "\n",
    "\n",
    "def _low_confidence_spans(\n",
    "    tokens: Sequence[str],\n",
    "    log_probs: Sequence[float],\n",
    "    min_prob: float,\n",
    "    min_token_gap: int,\n",
    "    num_pad_tokens: int,\n",
    ") -> List[str]:\n",
    "    _low_idx = np.where(np.exp(log_probs) < min_prob)[0]\n",
    "    low_idx = [i for i in _low_idx if re.search(r\"\\w\", tokens[i])]\n",
    "    if len(low_idx) == 0:\n",
    "        return []\n",
    "    spans = [[low_idx[0], low_idx[0] + num_pad_tokens + 1]]\n",
    "    for i, idx in enumerate(low_idx[1:]):\n",
    "        end = idx + num_pad_tokens + 1\n",
    "        if idx - low_idx[i] < min_token_gap:\n",
    "            spans[-1][1] = end\n",
    "        else:\n",
    "            spans.append([idx, end])\n",
    "    return [\"\".join(tokens[start:end]) for start, end in spans]\n",
    "\n",
    "\n",
    "class FlareChain(Chain):\n",
    "    \"\"\"Chain that combines a retriever, a question generator,\n",
    "    and a response generator.\n",
    "\n",
    "    See [Active Retrieval Augmented Generation](https://arxiv.org/abs/2305.06983) paper.\n",
    "    \"\"\"\n",
    "\n",
    "    question_generator_chain: Runnable\n",
    "    \"\"\"Chain that generates questions from uncertain spans.\"\"\"\n",
    "    response_chain: Runnable\n",
    "    \"\"\"Chain that generates responses from user input and context.\"\"\"\n",
    "    output_parser: FinishedOutputParser = Field(default_factory=FinishedOutputParser)\n",
    "    \"\"\"Parser that determines whether the chain is finished.\"\"\"\n",
    "    retriever: VectorStore\n",
    "    \"\"\"Retriever that retrieves relevant documents from a user input.\"\"\"\n",
    "    min_prob: float = 0.2\n",
    "    \"\"\"Minimum probability for a token to be considered low confidence.\"\"\"\n",
    "    min_token_gap: int = 5\n",
    "    \"\"\"Minimum number of tokens between two low confidence spans.\"\"\"\n",
    "    num_pad_tokens: int = 2\n",
    "    \"\"\"Number of tokens to pad around a low confidence span.\"\"\"\n",
    "    max_iter: int = 10\n",
    "    \"\"\"Maximum number of iterations.\"\"\"\n",
    "    start_with_retrieval: bool = True\n",
    "    \"\"\"Whether to start with retrieval.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        \"\"\"Input keys for the chain.\"\"\"\n",
    "        return [\"user_input\"]\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        \"\"\"Output keys for the chain.\"\"\"\n",
    "        return [\"response\"]\n",
    "\n",
    "    def _do_generation(\n",
    "        self,\n",
    "        questions: List[str],\n",
    "        user_input: str,\n",
    "        response: str,\n",
    "        _run_manager: CallbackManagerForChainRun,\n",
    "    ) -> Tuple[str, bool]:\n",
    "        callbacks = _run_manager.get_child()\n",
    "        docs = []\n",
    "        for question in questions:\n",
    "            # d = self.retriever.invoke(question)\n",
    "            d = self.retriever.similarity_search(question,k=5)\n",
    "            # d = execute_curl_request(question)\n",
    "            docs.extend(d)\n",
    "            # question_context_pair[question] = d\n",
    "            if response.lower() == \"\":\n",
    "                memory_dict[\"0\"] = {}\n",
    "                memory_dict[\"0\"][question] = d\n",
    "            else:\n",
    "                memory_dict[response] = {}\n",
    "                memory_dict[response][question] = d\n",
    "        context = \"\\n\\n\".join(d.page_content for d in docs)\n",
    "        result = self.response_chain.invoke(\n",
    "            {\n",
    "                \"user_input\": user_input,\n",
    "                \"context\": context,\n",
    "                \"response\": response,\n",
    "            },\n",
    "            {\"callbacks\": callbacks},\n",
    "        )\n",
    "        if isinstance(result, AIMessage):\n",
    "            result = result.content\n",
    "        marginal, finished = self.output_parser.parse(result)\n",
    "        return marginal, finished\n",
    "\n",
    "    def _do_retrieval(\n",
    "        self,\n",
    "        low_confidence_spans: List[str],\n",
    "        _run_manager: CallbackManagerForChainRun,\n",
    "        user_input: str,\n",
    "        response: str,\n",
    "        initial_response: str,\n",
    "    ) -> Tuple[str, bool]:\n",
    "        question_gen_inputs = [\n",
    "            {\n",
    "                \"user_input\": user_input,\n",
    "                \"current_response\": initial_response,\n",
    "                \"uncertain_span\": span,\n",
    "            }\n",
    "            for span in low_confidence_spans\n",
    "        ]\n",
    "        callbacks = _run_manager.get_child()\n",
    "        if isinstance(self.question_generator_chain, LLMChain):\n",
    "            question_gen_outputs = self.question_generator_chain.apply(\n",
    "                question_gen_inputs, callbacks=callbacks\n",
    "            )\n",
    "            questions = [\n",
    "                output[self.question_generator_chain.output_keys[0]]\n",
    "                for output in question_gen_outputs\n",
    "            ]\n",
    "        else:\n",
    "            questions = self.question_generator_chain.batch(\n",
    "                question_gen_inputs, config={\"callbacks\": callbacks}\n",
    "            )\n",
    "        _run_manager.on_text(\n",
    "            f\"Generated Questions: {questions}\", color=\"yellow\", end=\"\\n\"\n",
    "        )\n",
    "        return self._do_generation(questions, user_input, response, _run_manager)\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        inputs: Dict[str, Any],\n",
    "        run_manager: Optional[CallbackManagerForChainRun] = None,\n",
    "    ) -> Dict[str, Any]:\n",
    "        _run_manager = run_manager or CallbackManagerForChainRun.get_noop_manager()\n",
    "\n",
    "        user_input = inputs[self.input_keys[0]]\n",
    "\n",
    "        response = \"\"\n",
    "\n",
    "        for i in range(self.max_iter):\n",
    "            _run_manager.on_text(\n",
    "                f\"Current Response: {response}\", color=\"blue\", end=\"\\n\"\n",
    "            )\n",
    "            _input = {\"user_input\": user_input, \"context\": \"\", \"response\": response}\n",
    "            tokens, log_probs = _extract_tokens_and_log_probs(\n",
    "                self.response_chain.invoke(\n",
    "                    _input, {\"callbacks\": _run_manager.get_child()}\n",
    "                )\n",
    "            )\n",
    "            low_confidence_spans = _low_confidence_spans(\n",
    "                tokens,\n",
    "                log_probs,\n",
    "                self.min_prob,\n",
    "                self.min_token_gap,\n",
    "                self.num_pad_tokens,\n",
    "            )\n",
    "            initial_response = response.strip() + \" \" + \"\".join(tokens)\n",
    "            if not low_confidence_spans:\n",
    "                response = initial_response\n",
    "                final_response, finished = self.output_parser.parse(response)\n",
    "                if finished:\n",
    "                    return {self.output_keys[0]: final_response}\n",
    "                continue\n",
    "\n",
    "            marginal, finished = self._do_retrieval(\n",
    "                low_confidence_spans,\n",
    "                _run_manager,\n",
    "                user_input,\n",
    "                response,\n",
    "                initial_response,\n",
    "            )\n",
    "            response = response.strip() + \" \" + marginal\n",
    "            if finished:\n",
    "                break\n",
    "        # current_response = response\n",
    "        return {self.output_keys[0]: response}\n",
    "\n",
    "    @classmethod\n",
    "    def from_llm(\n",
    "        cls, llm: BaseLanguageModel, max_generation_len: int = 32, **kwargs: Any\n",
    "    ) -> FlareChain:\n",
    "        \"\"\"Creates a FlareChain from a language model.\n",
    "\n",
    "        Args:\n",
    "            llm: Language model to use.\n",
    "            max_generation_len: Maximum length of the generated response.\n",
    "            kwargs: Additional arguments to pass to the constructor.\n",
    "\n",
    "        Returns:\n",
    "            FlareChain class with the given language model.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from langchain_openai import ChatOpenAI\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"OpenAI is required for FlareChain. \"\n",
    "                \"Please install langchain-openai.\"\n",
    "                \"pip install langchain-openai\"\n",
    "            )\n",
    "        llm = ChatOpenAI(model=\"gpt-4o\",max_tokens=max_generation_len, logprobs=True, temperature=0)\n",
    "        response_chain = PROMPT | llm\n",
    "        question_gen_chain = QUESTION_GENERATOR_PROMPT | llm | StrOutputParser()\n",
    "        return cls(\n",
    "            question_generator_chain=question_gen_chain,\n",
    "            response_chain=response_chain,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "# Flare Chain\n",
    "\n",
    "flare = FlareChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=client,\n",
    "    max_generation_len=300,\n",
    "    min_prob=0.45,\n",
    "    max_iter=10,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "# Define Function for Gradio Interface\n",
    "def generate_flare_output(input_text):\n",
    "    output = flare.run(input_text)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"math_operations\",\n",
    "            \"description\": \"Performs basic arithmetic operations: addition, subtraction, multiplication, and division. Use this for mathematical calculations.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"a\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"The first number.\"\n",
    "                    },\n",
    "                    \"b\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"The second number.\"\n",
    "                    },\n",
    "                    \"operation_code\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The operation to perform (1: Add, 2: Subtract, 3: Multiply, 4: Divide).\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"a\", \"b\", \"operation_code\"],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"parse_pdf_to_json\",\n",
    "            \"description\": \"Parses the contents of a PDF file and saves it in a structured JSON format. Use this for converting PDFs to machine-readable data.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"pdf_path\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The file path of the PDF to parse.\"\n",
    "                    },\n",
    "                    \"json_path\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The file path to save the output JSON.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"pdf_path\", \"json_path\"],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"search_duckduckgo\",\n",
    "            \"description\": \"Performs a web search using DuckDuckGo. Use this to retrieve search results for a specific keyword or phrase.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"keyword\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The search term or phrase.\"\n",
    "                    },\n",
    "                    \"max_results\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"description\": \"The maximum number of search results to retrieve.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"keyword\"],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Functions\n",
    "###\n",
    "def create_messages(query):\n",
    "  return [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a skilled and empathetic assistant, adept at solving user problems efficiently and accurately. Use your deep understanding of context, reasoning, and tools at your disposal to provide solutions tailored to the user's needs. When appropriate, ask clarifying questions to ensure a thorough response. Respond concisely and clearly, maintaining a friendly and professional tone. Always prioritize the user's objectives while explaining your reasoning where necessary.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": query\n",
    "        }\n",
    "    ]\n",
    "\n",
    "def classify_query(query):\n",
    "    prompt = f\"\"\"\n",
    "    The following query needs to be classified into one of two categories: 'Finance' or 'Legal'.\n",
    "\n",
    "    Query: \"{query}\"\n",
    "\n",
    "    Categories:\n",
    "    1. Finance\n",
    "    2. Legal\n",
    "\n",
    "    Please classify the query into one of the categories. Just give answers in one word.\n",
    "    \"\"\"\n",
    "\n",
    "    msg = create_messages(prompt)\n",
    "\n",
    "\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=msg,\n",
    "        )\n",
    "\n",
    "        classification = response.choices[0].message.content\n",
    "\n",
    "        return classification\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "###\n",
    "def is_satisfied(query,response):\n",
    "    evaluation_prompt = f\"\"\"\n",
    "    Evaluate the following:\n",
    "\n",
    "    User Query: '{query}'\n",
    "    LLM Response: '{response}'\n",
    "\n",
    "    Does the LLM response fully and accurately satisfy the user's query based on the provided context?\n",
    "\n",
    "    Respond with \"Yes\" if the response completely satisfies the user's query. Respond with \"No\" if it does not.\n",
    "    \"\"\"\n",
    "    msg = create_messages(evaluation_prompt)\n",
    "\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=msg,\n",
    "        )\n",
    "\n",
    "        classification = response.choices[0].message.content\n",
    "\n",
    "        return classification\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "###\n",
    "def updated_reponse_generator(query,extra_context,prev_response):\n",
    "    new_prompt = f\"\"\"\n",
    "    Using the provided resource below, improve or generate a complete response to the user query:\n",
    "\n",
    "    Resource: {extra_context}\n",
    "\n",
    "    User Query: {query}\n",
    "\n",
    "    Previous Response: {prev_response}\n",
    "\n",
    "    Consider the following:\n",
    "    1. Ensure the resource is fully utilized to address the query.\n",
    "    2. If the previous response is incomplete or incorrect, provide a revised and complete response.\n",
    "    3. Maintain a concise, user-friendly, and accurate tone in your response.\n",
    "    \"\"\"\n",
    "\n",
    "    msg = create_messages(new_prompt)\n",
    "\n",
    "    try:\n",
    "        response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=msg,\n",
    "        )\n",
    "\n",
    "        classification = response.choices[0].message.content\n",
    "\n",
    "        return classification\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "###\n",
    "def tool_calls(response):\n",
    "    # Extract `choices` and `tool_calls`\n",
    "    choices = response.choices  # Access the choices attribute\n",
    "    parsed_data = []\n",
    "\n",
    "    for choice in choices:\n",
    "        for tool_call in choice.message.tool_calls:\n",
    "            # print(tool_call.function.arguments)\n",
    "            arguments = json.loads(tool_call.function.arguments)\n",
    "            tool_name = tool_call.function.name\n",
    "            parsed_data.append({\"tool_name\": tool_name, \"arguments\": arguments})\n",
    "\n",
    "    return parsed_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool Functions\n",
    "\n",
    "def search_duckduckgo(keyword, max_results=5):\n",
    "    time.sleep(60)\n",
    "    \"Performs a web search using DuckDuckGo. Use this to retrieve search results for a specific keyword or phrase.\"\n",
    "    print(f\"Tool called with input: {keyword}\")\n",
    "    search = DuckDuckGoSearchRun()\n",
    "    # results = DDGS().text(keyword, max_results=max_results)\n",
    "    results = search.invoke(keyword, max_results=max_results)\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "def parse_pdf_to_json(pdf_path, json_path):\n",
    "    \"\"\"Parses a PDF file and stores its contents in a JSON file.\"\"\"\n",
    "\n",
    "    pdf_file = open(pdf_path, 'rb')\n",
    "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for page_num in range(len(pdf_reader.pages)):\n",
    "        page = pdf_reader.pages[page_num]\n",
    "        text = page.extract_text()\n",
    "        data.append({\"page\": page_num + 1, \"content\": text})\n",
    "\n",
    "    pdf_file.close()\n",
    "\n",
    "    with open(json_path, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "def math_operations(a, b, operation_code):\n",
    "    if operation_code == 1:\n",
    "        return a + b\n",
    "    elif operation_code == 2:\n",
    "        return a - b\n",
    "    elif operation_code == 3:\n",
    "        return a * b\n",
    "    elif operation_code == 4:\n",
    "        return a / b if b != 0 else \"Error: Division by zero\"\n",
    "    else:\n",
    "        return \"Error: Invalid operation code\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flare Agent Function\n",
    "\n",
    "def flare_response_generator(query):\n",
    "\n",
    "  #FLARE output\n",
    "  response = generate_flare_output(query)\n",
    "\n",
    "  #check if LLM is satisfied with answer\n",
    "  status = is_satisfied(query,response)\n",
    "#   print(f\"STATUS :  {status}\")\n",
    "  if status.lower() == 'yes':\n",
    "    return response\n",
    "\n",
    "  elif status.lower() == 'no':\n",
    "    test_prompt = f\"\"\"\n",
    "    The LLM response to the user query \"{query}\" was \"{response}\".\n",
    "    Ensure the continuation of the response uses relevant tool calls to provide accurate and actionable outputs.\n",
    "    \"\"\"\n",
    "    msg = create_messages(test_prompt)\n",
    "    further_response = openai.chat.completions.create(\n",
    "            model=\"gpt-4o\",\n",
    "            messages=msg,\n",
    "            tools=tools,\n",
    "          )\n",
    "    # print(further_response)\n",
    "    # tool_name,keyword,max_results = tool_calls(further_response)\n",
    "    try:\n",
    "      parsed_data = tool_calls(further_response)\n",
    "      for data in parsed_data:\n",
    "        if data['tool_name'].lower() == 'search_duckduckgo':\n",
    "          extra_context = search_duckduckgo(data['arguments']['keyword'],data['arguments']['max_results'])\n",
    "        elif data['tool_name'].lower() == 'math_operations':\n",
    "          extra_context = parse_pdf_to_json(data['arguments']['pdf_path'],data['arguments']['json_path'])\n",
    "        elif data['tool_name'].lower() == 'math_operations':\n",
    "          extra_context = math_operations(data['arguments']['a'],data['arguments']['b'],data['arguments']['operation_code'])\n",
    "\n",
    "\n",
    "      final_response = updated_reponse_generator(query,extra_context,response)\n",
    "\n",
    "      return final_response\n",
    "    except:\n",
    "      return response \n",
    "\n",
    "    \n",
    "  \n",
    "# Flare agent function -- NO TOOLS --\n",
    "\n",
    "def flare_only_response_generator(query):\n",
    "\n",
    "    #FLARE output\n",
    "    response = generate_flare_output(query)\n",
    "\n",
    "    return response\n",
    "# print(f\"#############################{memory_dict}\")\n",
    "# Main Agent\n",
    "\n",
    "def Agent(query):\n",
    "  query_type = classify_query(query)\n",
    "  print(query_type)\n",
    "  if query_type.lower() == 'finance':\n",
    "    return flare_response_generator(query)\n",
    "  elif query_type.lower() == 'legal':\n",
    "    return flare_response_generator(query)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINANCE INFERENCE AND EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MULTIHOP INFERENCE ###\n",
    "json_file_path = r\"datasets\\multihop_finance.json\"\n",
    "json_file = r'notebooks\\FLARE_on_multihop_finance.json' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "queries = json.load(open(json_file_path))\n",
    "for i in tqdm(range(40), desc=\"Processing queries\"):\n",
    "    query = queries[i]['question']\n",
    "    ground_truth_answer = queries[i]['answer']\n",
    "    try:\n",
    "        response = Agent(query)\n",
    "    except IndexError:\n",
    "        print(f\"IndexError at query {i}: Missing parentheses or unexpected format in FLARE response.\")\n",
    "        response = None\n",
    "    data.append({\n",
    "        'query': query,\n",
    "        'ground_truth': ground_truth_answer,\n",
    "        'response': response\n",
    "    })\n",
    "    with open(json_file, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MULTIHOP EVAL ###\n",
    "\n",
    "with open(json_file, 'r') as file:\n",
    "    question_chunks_data = json.load(file)\n",
    "\n",
    "# F1 Score , Precision , Recall\n",
    "def normalize_answer(s):\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    normalized_prediction = normalize_answer(prediction)\n",
    "    normalized_ground_truth = normalize_answer(ground_truth)\n",
    "\n",
    "    ZERO_METRIC = (0, 0, 0)\n",
    "\n",
    "    if normalized_prediction in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "        return ZERO_METRIC\n",
    "    if normalized_ground_truth in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "        return ZERO_METRIC\n",
    "\n",
    "    prediction_tokens = normalized_prediction.split()\n",
    "    ground_truth_tokens = normalized_ground_truth.split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return ZERO_METRIC\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1, precision, recall\n",
    "\n",
    "def calculate_cosine_similarity(text1, text2):\n",
    "    if not text1 or not text2:  # Handle empty input\n",
    "        return 0.0\n",
    "    vectorizer = CountVectorizer().fit_transform([text1, text2])\n",
    "    vectors = vectorizer.toarray()\n",
    "    return cosine_similarity(vectors)[0, 1]\n",
    "\n",
    "total_f1 = 0\n",
    "total_precision = 0\n",
    "total_recall = 0\n",
    "total_cosine_sim=0\n",
    "count = 0\n",
    "\n",
    "for question_entry in question_chunks_data:\n",
    "    question = question_entry['query']\n",
    "    ground_truth = question_entry['ground_truth']\n",
    "    response = question_entry['response']\n",
    "\n",
    "    if response:\n",
    "        f1, precision, recall = f1_score(response, ground_truth)\n",
    "        cosine_sim=calculate_cosine_similarity(response,ground_truth)\n",
    "        total_cosine_sim+=cosine_sim\n",
    "        total_f1 += f1\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        count += 1\n",
    "avg_f1 = total_f1 / count if count else 0\n",
    "avg_precision = total_precision / count if count else 0\n",
    "avg_recall = total_recall / count if count else 0\n",
    "avg_cosine_sim=total_cosine_sim/ count if count else 0\n",
    "\n",
    "print(f\"Average F1 Score: {avg_f1:.4f}\")\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average Cosine Similarity: {avg_cosine_sim:.4f}\")\n",
    "\n",
    "# LLM Eval\n",
    "llm = Groq()\n",
    "LLM_Score = 0\n",
    "num_rows = len(question_chunks_data)\n",
    "\n",
    "for i in range(num_rows):\n",
    "    answer = question_chunks_data[i]['ground_truth']\n",
    "    answer_predicted = question_chunks_data[i]['response']\n",
    "    ques = question_chunks_data[i]['query']\n",
    "    chat_completion = llm.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"Evaluate the semantic similarity between the following two answers with respect to some question. Output only a single floating-point number between 0 and 1, where 0 indicates no similarity and 1 indicates identical meaning. Respond with only the number and no other text, explanations, or symbols:\n",
    "Question: {ques}\n",
    "\n",
    "Text A: {answer}\n",
    "\n",
    "Text B: {answer_predicted}\"\"\",\n",
    "            }\n",
    "        ],\n",
    "        model=\"llama-3.1-70b-versatile\",\n",
    "        stream=False,\n",
    "    )\n",
    "    score = float(chat_completion.choices[0].message.content.strip())\n",
    "    LLM_Score += score\n",
    "average_llm_score = LLM_Score / num_rows if num_rows > 0 else 0\n",
    "print(f\"Average LLM Similarity Score: {average_llm_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SINGLEHOP FINANCIAL INFERENCE ###\n",
    "json_file_path = r\"datasets\\singlehop_finance.json\"\n",
    "json_file = r'notebooks\\FLARE_on_singlehop_finance.json' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "queries = json.load(open(json_file_path))\n",
    "for i in tqdm(range(150), desc=\"Processing queries\"):\n",
    "    query = queries[i]['question']\n",
    "    ground_truth_answer = queries[i]['answer']\n",
    "    try:\n",
    "        response = Agent(query)\n",
    "    except IndexError:\n",
    "        print(f\"IndexError at query {i}: Missing parentheses or unexpected format in FLARE response.\")\n",
    "        response = None\n",
    "    data.append({\n",
    "        'query': query,\n",
    "        'ground_truth': ground_truth_answer,\n",
    "        'response': response\n",
    "    })\n",
    "    with open(json_file, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SINGLEHOP FINANCIAL EVAL ###\n",
    "\n",
    "with open(json_file, 'r') as file:\n",
    "    question_chunks_data = json.load(file)\n",
    "\n",
    "# F1 Score , Precision , Recall\n",
    "def normalize_answer(s):\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    normalized_prediction = normalize_answer(prediction)\n",
    "    normalized_ground_truth = normalize_answer(ground_truth)\n",
    "\n",
    "    ZERO_METRIC = (0, 0, 0)\n",
    "\n",
    "    if normalized_prediction in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "        return ZERO_METRIC\n",
    "    if normalized_ground_truth in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "        return ZERO_METRIC\n",
    "\n",
    "    prediction_tokens = normalized_prediction.split()\n",
    "    ground_truth_tokens = normalized_ground_truth.split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return ZERO_METRIC\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1, precision, recall\n",
    "\n",
    "def calculate_cosine_similarity(text1, text2):\n",
    "    if not text1 or not text2:  # Handle empty input\n",
    "        return 0.0\n",
    "    vectorizer = CountVectorizer().fit_transform([text1, text2])\n",
    "    vectors = vectorizer.toarray()\n",
    "    return cosine_similarity(vectors)[0, 1]\n",
    "\n",
    "total_f1 = 0\n",
    "total_precision = 0\n",
    "total_recall = 0\n",
    "total_cosine_sim=0\n",
    "count = 0\n",
    "\n",
    "for question_entry in question_chunks_data:\n",
    "    question = question_entry['query']\n",
    "    ground_truth = question_entry['ground_truth']\n",
    "    response = question_entry['response']\n",
    "\n",
    "    if response:\n",
    "        f1, precision, recall = f1_score(response, ground_truth)\n",
    "        cosine_sim=calculate_cosine_similarity(response,ground_truth)\n",
    "        total_cosine_sim+=cosine_sim\n",
    "        total_f1 += f1\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        count += 1\n",
    "avg_f1 = total_f1 / count if count else 0\n",
    "avg_precision = total_precision / count if count else 0\n",
    "avg_recall = total_recall / count if count else 0\n",
    "avg_cosine_sim=total_cosine_sim/ count if count else 0\n",
    "\n",
    "print(f\"Average F1 Score: {avg_f1:.4f}\")\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average Cosine Similarity: {avg_cosine_sim:.4f}\")\n",
    "\n",
    "# LLM Eval\n",
    "llm = Groq()\n",
    "LLM_Score = 0\n",
    "num_rows = len(question_chunks_data)\n",
    "\n",
    "for i in range(num_rows):\n",
    "    answer = question_chunks_data[i]['ground_truth']\n",
    "    answer_predicted = question_chunks_data[i]['response']\n",
    "    ques = question_chunks_data[i]['query']\n",
    "    chat_completion = llm.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"Evaluate the semantic similarity between the following two answers with respect to some question. Output only a single floating-point number between 0 and 1, where 0 indicates no similarity and 1 indicates identical meaning. Respond with only the number and no other text, explanations, or symbols:\n",
    "Question: {ques}\n",
    "\n",
    "Text A: {answer}\n",
    "\n",
    "Text B: {answer_predicted}\"\"\",\n",
    "            }\n",
    "        ],\n",
    "        model=\"llama-3.1-70b-versatile\",\n",
    "        stream=False,\n",
    "    )\n",
    "    score = float(chat_completion.choices[0].message.content.strip())\n",
    "    LLM_Score += score\n",
    "average_llm_score = LLM_Score / num_rows if num_rows > 0 else 0\n",
    "print(f\"Average LLM Similarity Score: {average_llm_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEGAL INFERENCE AND EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = PathwayVectorClient(url=\"http://localhost:8001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CUAD INFERENCE ###\n",
    "json_file_path = r\"datasets\\CUAD_LBRAG.json\"\n",
    "json_file = r'notebooks\\FLARE_on_CUAD_LBRAG.json' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "queries = json.load(open(json_file_path))\n",
    "for i in tqdm(range(100), desc=\"Processing queries\"):\n",
    "    query = queries[i]['question']\n",
    "    ground_truth_answer = queries[i]['answer']\n",
    "    try:\n",
    "        response = Agent(query)\n",
    "    except IndexError:\n",
    "        print(f\"IndexError at query {i}: Missing parentheses or unexpected format in FLARE response.\")\n",
    "        response = None\n",
    "    data.append({\n",
    "        'query': query,\n",
    "        'ground_truth': ground_truth_answer,\n",
    "        'response': response\n",
    "    })\n",
    "    with open(json_file, 'w') as f:\n",
    "        json.dump(data, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CUAD EVAL ###\n",
    "\n",
    "with open(json_file, 'r') as file:\n",
    "    question_chunks_data = json.load(file)\n",
    "\n",
    "# F1 Score , Precision , Recall\n",
    "def normalize_answer(s):\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def f1_score(prediction, ground_truth):\n",
    "    normalized_prediction = normalize_answer(prediction)\n",
    "    normalized_ground_truth = normalize_answer(ground_truth)\n",
    "\n",
    "    ZERO_METRIC = (0, 0, 0)\n",
    "\n",
    "    if normalized_prediction in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "        return ZERO_METRIC\n",
    "    if normalized_ground_truth in ['yes', 'no', 'noanswer'] and normalized_prediction != normalized_ground_truth:\n",
    "        return ZERO_METRIC\n",
    "\n",
    "    prediction_tokens = normalized_prediction.split()\n",
    "    ground_truth_tokens = normalized_ground_truth.split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return ZERO_METRIC\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1, precision, recall\n",
    "\n",
    "def calculate_cosine_similarity(text1, text2):\n",
    "    if not text1 or not text2:  # Handle empty input\n",
    "        return 0.0\n",
    "    vectorizer = CountVectorizer().fit_transform([text1, text2])\n",
    "    vectors = vectorizer.toarray()\n",
    "    return cosine_similarity(vectors)[0, 1]\n",
    "\n",
    "total_f1 = 0\n",
    "total_precision = 0\n",
    "total_recall = 0\n",
    "total_cosine_sim=0\n",
    "count = 0\n",
    "\n",
    "for question_entry in question_chunks_data:\n",
    "    question = question_entry['query']\n",
    "    ground_truth = question_entry['ground_truth']\n",
    "    response = question_entry['response']\n",
    "\n",
    "    if response:\n",
    "        f1, precision, recall = f1_score(response, ground_truth)\n",
    "        cosine_sim=calculate_cosine_similarity(response,ground_truth)\n",
    "        total_cosine_sim+=cosine_sim\n",
    "        total_f1 += f1\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        count += 1\n",
    "avg_f1 = total_f1 / count if count else 0\n",
    "avg_precision = total_precision / count if count else 0\n",
    "avg_recall = total_recall / count if count else 0\n",
    "avg_cosine_sim=total_cosine_sim/ count if count else 0\n",
    "\n",
    "print(f\"Average F1 Score: {avg_f1:.4f}\")\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average Cosine Similarity: {avg_cosine_sim:.4f}\")\n",
    "\n",
    "# LLM Eval\n",
    "llm = Groq()\n",
    "LLM_Score = 0\n",
    "num_rows = len(question_chunks_data)\n",
    "\n",
    "for i in range(num_rows):\n",
    "    answer = question_chunks_data[i]['ground_truth']\n",
    "    answer_predicted = question_chunks_data[i]['response']\n",
    "    ques = question_chunks_data[i]['query']\n",
    "    chat_completion = llm.chat.completions.create(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"Evaluate the semantic similarity between the following two answers with respect to some question. Output only a single floating-point number between 0 and 1, where 0 indicates no similarity and 1 indicates identical meaning. Respond with only the number and no other text, explanations, or symbols:\n",
    "Question: {ques}\n",
    "\n",
    "Text A: {answer}\n",
    "\n",
    "Text B: {answer_predicted}\"\"\",\n",
    "            }\n",
    "        ],\n",
    "        model=\"llama-3.1-70b-versatile\",\n",
    "        stream=False,\n",
    "    )\n",
    "    score = float(chat_completion.choices[0].message.content.strip())\n",
    "    LLM_Score += score\n",
    "average_llm_score = LLM_Score / num_rows if num_rows > 0 else 0\n",
    "print(f\"Average LLM Similarity Score: {average_llm_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
