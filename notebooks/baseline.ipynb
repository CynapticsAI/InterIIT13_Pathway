{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openai\n",
    "!pip install scikit-learn\n",
    "!pip install groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import urllib.parse\n",
    "import json\n",
    "\n",
    "def execute_curl_request(query, k=3):\n",
    "    \"\"\"\n",
    "    description: This function retrieves relevant information from a database consisting of financial information\"\"\"\n",
    "    # URL encode the query\n",
    "    encoded_query = urllib.parse.quote(query)\n",
    "    url = f\"http://localhost:8000/v1/retrieve?query={encoded_query}&k={k}\"\n",
    "    \n",
    "    # Construct the curl command\n",
    "    command = [\n",
    "        \"curl\", \"-X\", \"GET\", url, \"-H\", \"accept: */*\"\n",
    "    ]\n",
    "    \n",
    "    # Execute the command\n",
    "    try:\n",
    "        result = subprocess.run(command, capture_output=True, text=True, check=True)\n",
    "        response = result.stdout\n",
    "        \n",
    "        # Parse the JSON response\n",
    "        data = json.loads(response)\n",
    "        \n",
    "        # Extract and print only the \"text\" field\n",
    "        for item in data:\n",
    "            ptext=item.get(\"text\")\n",
    "            # print(\"Text:\\n\", item.get(\"text\"))\n",
    "            i=1\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Error:\", e.stderr)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Failed to parse JSON response.\")\n",
    "    s=\"Text:\\n\"+item.get(\"text\")+\"done\"\n",
    "    return s\n",
    "\n",
    "def execute_curl_request_legal(query, k=3):\n",
    "    \"\"\"\n",
    "    description: This function retrieves relevant information from a database consisting of financial information\"\"\"\n",
    "    # URL encode the query\n",
    "    encoded_query = urllib.parse.quote(query)\n",
    "    url = f\"http://localhost:8001/v1/retrieve?query={encoded_query}&k={k}\"\n",
    "    \n",
    "    # Construct the curl command\n",
    "    command = [\n",
    "        \"curl\", \"-X\", \"GET\", url, \"-H\", \"accept: */*\"\n",
    "    ]\n",
    "    \n",
    "    # Execute the command\n",
    "    try:\n",
    "        result = subprocess.run(command, capture_output=True, text=True, check=True)\n",
    "        response = result.stdout\n",
    "        \n",
    "        # Parse the JSON response\n",
    "        data = json.loads(response)\n",
    "        \n",
    "        # Extract and print only the \"text\" field\n",
    "        for item in data:\n",
    "            ptext=item.get(\"text\")\n",
    "            # print(\"Text:\\n\", item.get(\"text\"))\n",
    "            i=1\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Error:\", e.stderr)\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Failed to parse JSON response.\")\n",
    "    s=\"Text:\\n\"+item.get(\"text\")+\"done\"\n",
    "    return s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SET KEYS ###\n",
    "import os\n",
    "os.environ['OPENAI_API_KEY']='Your OpenAI API Key'\n",
    "os.environ['GROQ_API_KEY'] = \"Your GROQ API Key\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### INFERENCE FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "def process_questions_with_context_and_gpt(\n",
    "    input_json_path, \n",
    "    output_json_path, \n",
    "    model=\"gpt-4o-mini\",\n",
    "    k=3\n",
    "):\n",
    "    \"\"\"\n",
    "    Processes a JSON file containing questions, retrieves contexts, \n",
    "    and generates answers using GPT. Outputs the final JSON with question, context, \n",
    "    and answer pairs.\n",
    "\n",
    "    Args:\n",
    "        input_json_path (str): Path to the input JSON file with questions and ground truth.\n",
    "        output_json_path (str): Path to save the output JSON with questions, contexts, and answers.\n",
    "        model (str): The GPT model to use for generating responses.\n",
    "        k (int): Number of contexts to retrieve per question.\n",
    "    \"\"\"\n",
    "    # Initialize the OpenAI client\n",
    "    client = OpenAI()\n",
    "\n",
    "    # Load the input JSON data\n",
    "    with open(input_json_path, 'r', encoding='utf-8') as file:\n",
    "        questions_data = json.load(file)\n",
    "\n",
    "    # Initialize the output list\n",
    "    output_data = []\n",
    "\n",
    "    # Iterate through each question in the input JSON\n",
    "    for question_entry in questions_data:\n",
    "        question = question_entry.get(\"question\", \"\").strip()  # Extract and sanitize the question\n",
    "        if not question:\n",
    "            continue  # Skip if the question is empty\n",
    "\n",
    "        try:\n",
    "            # Retrieve contexts using the `execute_curl_request` function\n",
    "            contexts_raw = execute_curl_request(query=question, k=k)\n",
    "            context_list = contexts_raw.split(\"done\")[:-1]  # Process into a list\n",
    "\n",
    "            # Prepare the prompt for GPT\n",
    "            context_text = \"\\n\".join(context_list) if context_list else \"No context provided.\"\n",
    "            prompt = f\"I am giving you a query and the necessary context to answer it.\\n\\nQuery: {question}\\n\\nContext:\\n{context_text} \\n be concise, if the answer is not in the context provided print that\"\n",
    "\n",
    "            # Get the response from GPT-4o-mini in streaming mode\n",
    "            response_text = \"\"\n",
    "            stream = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                stream=True,\n",
    "            )\n",
    "            for chunk in stream:\n",
    "                delta_content = chunk.choices[0].delta.content\n",
    "                if delta_content:\n",
    "                    response_text += delta_content\n",
    "\n",
    "            # Append the processed data to the output list\n",
    "            output_data.append({\n",
    "                \"question\": question,\n",
    "                \"contexts\": context_list,\n",
    "                \"answer\": response_text.strip()\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question: {question}\\n{e}\")\n",
    "\n",
    "    # Save the output data to a JSON file\n",
    "    with open(output_json_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(output_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Processed {len(output_data)} questions. Output saved to {output_json_path}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATION FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Set up the Groq API\n",
    "\n",
    "Groq_API_Key = os.getenv('GROQ_API_KEY', None)\n",
    "\n",
    "if not Groq_API_Key:\n",
    "    raise ValueError(\"GROQ_API_KEY is not set.\")\n",
    "\n",
    "from groq import Groq\n",
    "llm = Groq()\n",
    "\n",
    "# Cosine similarity function\n",
    "def calculate_cosine_similarity(text1, text2):\n",
    "    if not text1 or not text2:  # Handle empty input\n",
    "        return 0.0\n",
    "    vectorizer = CountVectorizer().fit_transform([text1, text2])\n",
    "    vectors = vectorizer.toarray()\n",
    "    return cosine_similarity(vectors)[0, 1]\n",
    "\n",
    "# Text normalization function\n",
    "def normalize_answer(s):\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return ''.join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "# F1 Score calculation function\n",
    "def f1_score(prediction, ground_truth):\n",
    "    normalized_prediction = normalize_answer(prediction)\n",
    "    normalized_ground_truth = normalize_answer(ground_truth)\n",
    "\n",
    "    ZERO_METRIC = (0, 0, 0, 0.0)  # Include a default value for cosine similarity\n",
    "\n",
    "    # Handle binary answers separately\n",
    "    if normalized_prediction in ['yes', 'no', 'noanswer'] or normalized_ground_truth in ['yes', 'no', 'noanswer']:\n",
    "        if normalized_prediction != normalized_ground_truth:\n",
    "            return ZERO_METRIC\n",
    "\n",
    "    prediction_tokens = normalized_prediction.split()\n",
    "    ground_truth_tokens = normalized_ground_truth.split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "\n",
    "    if num_same == 0:\n",
    "        return ZERO_METRIC\n",
    "\n",
    "    precision = num_same / len(prediction_tokens)\n",
    "    recall = num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    cosine_sim = calculate_cosine_similarity(prediction, ground_truth)\n",
    "\n",
    "    return f1, precision, recall, cosine_sim\n",
    "\n",
    "\n",
    "# Load JSON data\n",
    "def load_json(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### MULTIHOP FINANCE INFERENCE  #####\n",
    "\n",
    "input_path = r\"datasets\\multihop_finance.json\"  # Input JSON file containing questions\n",
    "output_path = r\"notebooks\\baseline_on_multihop_finance.json\"  # Output JSON file\n",
    "\n",
    "# Process the questions, retrieve contexts, and generate answers\n",
    "process_questions_with_context_and_gpt(input_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### MULTIHOP FINANCE EVAL ####\n",
    "\n",
    "# Paths to the input files\n",
    "json_file_path = output_path\n",
    "ground_truth_path = input_path\n",
    "\n",
    "# Load data\n",
    "question_chunks_data = load_json(json_file_path)\n",
    "ground_truth_data = load_json(ground_truth_path)\n",
    "\n",
    "# Map questions to ground truth answers\n",
    "ground_truth_map = {entry['question']: entry['answer'] for entry in ground_truth_data}\n",
    "\n",
    "# Initialize metrics\n",
    "total_f1, total_precision, total_recall, total_cos, count = 0, 0, 0, 0, 0\n",
    "\n",
    "# Evaluate F1, precision, recall, cosine similarity\n",
    "for question_entry in question_chunks_data:\n",
    "    question = question_entry.get('question', \"\")\n",
    "    ground_truth = ground_truth_map.get(question, \"\")\n",
    "    chunk1_text = question_entry.get('answer', \"\")\n",
    "\n",
    "    if chunk1_text:  # Process only if answer exists\n",
    "        f1, precision, recall, cos = f1_score(chunk1_text, ground_truth)\n",
    "        total_f1 += f1\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_cos += cos\n",
    "        count += 1\n",
    "\n",
    "# Average metrics\n",
    "avg_f1 = total_f1 / count if count else 0\n",
    "avg_precision = total_precision / count if count else 0\n",
    "avg_recall = total_recall / count if count else 0\n",
    "avg_cosine_sim = total_cos / count if count else 0\n",
    "\n",
    "# Print results\n",
    "print(f\"Average F1 Score: {avg_f1:.4f}\")\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average Cosine Similarity: {avg_cosine_sim:.4f}\")\n",
    "\n",
    "# Evaluate with LLM\n",
    "LLM_Score = 0\n",
    "for question_entry in question_chunks_data:\n",
    "    query = question_entry.get('question', \"\")\n",
    "    reply = question_entry.get('answer', \"\")\n",
    "    answer = ground_truth_map.get(query, \"\")\n",
    "\n",
    "    if query and reply and answer:\n",
    "        chat_completion = llm.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"\"\"Evaluate the semantic similarity between the following two answers with respect to some question. Output only a single floating-point number between 0 and 1, where 0 indicates no similarity and 1 indicates identical meaning. Respond with only the number:\n",
    "Question: {query}\n",
    "Text A: {answer}\n",
    "Text B: {reply}\"\"\"\n",
    "                }\n",
    "            ],\n",
    "            model=\"llama-3.1-70b-versatile\",\n",
    "            stream=False,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            score = float(chat_completion.choices[0].message.content.strip())\n",
    "            LLM_Score += score\n",
    "        except ValueError:\n",
    "            print(\"Invalid LLM response for query:\", query)\n",
    "\n",
    "# Calculate and print average LLM similarity score\n",
    "average_llm_score = LLM_Score / count if count else 0\n",
    "print(f\"Average LLM Similarity Score: {average_llm_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### NVIDIA  #####\n",
    "\n",
    "input_path = r\"datasets\\singlehop_finance.json\"  # Input JSON file containing questions\n",
    "output_path = r\"notebooks\\baseline_on_singlehop_finance.json\"  # Output JSON file\n",
    "\n",
    "# Process the questions, retrieve contexts, and generate answers\n",
    "process_questions_with_context_and_gpt(input_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### NVIDIA EVAL ####\n",
    "\n",
    "# Paths to the input files\n",
    "json_file_path = output_path\n",
    "ground_truth_path = input_path\n",
    "\n",
    "# Load data\n",
    "question_chunks_data = load_json(json_file_path)\n",
    "ground_truth_data = load_json(ground_truth_path)\n",
    "\n",
    "# Map questions to ground truth answers\n",
    "ground_truth_map = {entry['question']: entry['answer'] for entry in ground_truth_data}\n",
    "\n",
    "# Initialize metrics\n",
    "total_f1, total_precision, total_recall, total_cos, count = 0, 0, 0, 0, 0\n",
    "\n",
    "# Evaluate F1, precision, recall, cosine similarity\n",
    "for question_entry in question_chunks_data:\n",
    "    question = question_entry.get('question', \"\")\n",
    "    ground_truth = ground_truth_map.get(question, \"\")\n",
    "    chunk1_text = question_entry.get('answer', \"\")\n",
    "\n",
    "    if chunk1_text:  # Process only if answer exists\n",
    "        f1, precision, recall, cos = f1_score(chunk1_text, ground_truth)\n",
    "        total_f1 += f1\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_cos += cos\n",
    "        count += 1\n",
    "\n",
    "# Average metrics\n",
    "avg_f1 = total_f1 / count if count else 0\n",
    "avg_precision = total_precision / count if count else 0\n",
    "avg_recall = total_recall / count if count else 0\n",
    "avg_cosine_sim = total_cos / count if count else 0\n",
    "\n",
    "# Print results\n",
    "print(f\"Average F1 Score: {avg_f1:.4f}\")\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average Cosine Similarity: {avg_cosine_sim:.4f}\")\n",
    "\n",
    "# Evaluate with LLM\n",
    "LLM_Score = 0\n",
    "for question_entry in question_chunks_data:\n",
    "    query = question_entry.get('question', \"\")\n",
    "    reply = question_entry.get('answer', \"\")\n",
    "    answer = ground_truth_map.get(query, \"\")\n",
    "\n",
    "    if query and reply and answer:\n",
    "        chat_completion = llm.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"\"\"Evaluate the semantic similarity between the following two answers with respect to some question. Output only a single floating-point number between 0 and 1, where 0 indicates no similarity and 1 indicates identical meaning. Respond with only the number:\n",
    "Question: {query}\n",
    "Text A: {answer}\n",
    "Text B: {reply}\"\"\"\n",
    "                }\n",
    "            ],\n",
    "            model=\"llama-3.1-70b-versatile\",\n",
    "            stream=False,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            score = float(chat_completion.choices[0].message.content.strip())\n",
    "            LLM_Score += score\n",
    "        except ValueError:\n",
    "            print(\"Invalid LLM response for query:\", query)\n",
    "\n",
    "# Calculate and print average LLM similarity score\n",
    "average_llm_score = LLM_Score / count if count else 0\n",
    "print(f\"Average LLM Similarity Score: {average_llm_score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEGAL INFERENCE AND EVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from openai import OpenAI\n",
    "\n",
    "def process_questions_with_context_and_gpt_legal(\n",
    "    input_json_path, \n",
    "    output_json_path, \n",
    "    model=\"gpt-4o-mini\",\n",
    "    k=3\n",
    "):\n",
    "    \"\"\"\n",
    "    Processes a JSON file containing questions, retrieves contexts, \n",
    "    and generates answers using GPT. Outputs the final JSON with question, context, \n",
    "    and answer pairs.\n",
    "\n",
    "    Args:\n",
    "        input_json_path (str): Path to the input JSON file with questions and ground truth.\n",
    "        output_json_path (str): Path to save the output JSON with questions, contexts, and answers.\n",
    "        model (str): The GPT model to use for generating responses.\n",
    "        k (int): Number of contexts to retrieve per question.\n",
    "    \"\"\"\n",
    "    # Initialize the OpenAI client\n",
    "    client = OpenAI()\n",
    "\n",
    "    # Load the input JSON data\n",
    "    with open(input_json_path, 'r', encoding='utf-8') as file:\n",
    "        questions_data = json.load(file)\n",
    "\n",
    "    # Initialize the output list\n",
    "    output_data = []\n",
    "\n",
    "    # Iterate through each question in the input JSON\n",
    "    for question_entry in questions_data:\n",
    "        question = question_entry.get(\"question\", \"\").strip()  # Extract and sanitize the question\n",
    "        if not question:\n",
    "            continue  # Skip if the question is empty\n",
    "\n",
    "        try:\n",
    "            # Retrieve contexts using the `execute_curl_request` function\n",
    "            contexts_raw = execute_curl_request_legal(query=question, k=k)\n",
    "            context_list = contexts_raw.split(\"done\")[:-1]  # Process into a list\n",
    "\n",
    "            # Prepare the prompt for GPT\n",
    "            context_text = \"\\n\".join(context_list) if context_list else \"No context provided.\"\n",
    "            prompt = f\"I am giving you a query and the necessary context to answer it.\\n\\nQuery: {question}\\n\\nContext:\\n{context_text} \\n be concise, if the answer is not in the context provided print that\"\n",
    "\n",
    "            # Get the response from GPT-4o-mini in streaming mode\n",
    "            response_text = \"\"\n",
    "            stream = client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                stream=True,\n",
    "            )\n",
    "            for chunk in stream:\n",
    "                delta_content = chunk.choices[0].delta.content\n",
    "                if delta_content:\n",
    "                    response_text += delta_content\n",
    "\n",
    "            # Append the processed data to the output list\n",
    "            output_data.append({\n",
    "                \"question\": question,\n",
    "                \"contexts\": context_list,\n",
    "                \"answer\": response_text.strip()\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question: {question}\\n{e}\")\n",
    "\n",
    "    # Save the output data to a JSON file\n",
    "    with open(output_json_path, 'w', encoding='utf-8') as file:\n",
    "        json.dump(output_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Processed {len(output_data)} questions. Output saved to {output_json_path}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CUAD INFERENCE  #####\n",
    "\n",
    "input_path = r\"datasets\\CUAD_LBRAG.json\"  # Input JSON file containing questions\n",
    "output_path = r\"notebooks\\baseline_on_CUAD_LBRAG.json\"  # Output JSON file\n",
    "\n",
    "# Process the questions, retrieve contexts, and generate answers\n",
    "process_questions_with_context_and_gpt_legal(input_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CUAD EVAL ####\n",
    "\n",
    "# Paths to the input files\n",
    "json_file_path = output_path\n",
    "ground_truth_path = input_path\n",
    "\n",
    "# Load data\n",
    "question_chunks_data = load_json(json_file_path)\n",
    "ground_truth_data = load_json(ground_truth_path)\n",
    "\n",
    "# Map questions to ground truth answers\n",
    "ground_truth_map = {entry['question']: entry['answer'] for entry in ground_truth_data}\n",
    "\n",
    "# Initialize metrics\n",
    "total_f1, total_precision, total_recall, total_cos, count = 0, 0, 0, 0, 0\n",
    "\n",
    "# Evaluate F1, precision, recall, cosine similarity\n",
    "for question_entry in question_chunks_data:\n",
    "    question = question_entry.get('question', \"\")\n",
    "    ground_truth = ground_truth_map.get(question, \"\")\n",
    "    chunk1_text = question_entry.get('answer', \"\")\n",
    "\n",
    "    if chunk1_text:  # Process only if answer exists\n",
    "        f1, precision, recall, cos = f1_score(chunk1_text, ground_truth)\n",
    "        total_f1 += f1\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_cos += cos\n",
    "        count += 1\n",
    "\n",
    "# Average metrics\n",
    "avg_f1 = total_f1 / count if count else 0\n",
    "avg_precision = total_precision / count if count else 0\n",
    "avg_recall = total_recall / count if count else 0\n",
    "avg_cosine_sim = total_cos / count if count else 0\n",
    "\n",
    "# Print results\n",
    "print(f\"Average F1 Score: {avg_f1:.4f}\")\n",
    "print(f\"Average Precision: {avg_precision:.4f}\")\n",
    "print(f\"Average Recall: {avg_recall:.4f}\")\n",
    "print(f\"Average Cosine Similarity: {avg_cosine_sim:.4f}\")\n",
    "\n",
    "# Evaluate with LLM\n",
    "LLM_Score = 0\n",
    "for question_entry in question_chunks_data:\n",
    "    query = question_entry.get('question', \"\")\n",
    "    reply = question_entry.get('answer', \"\")\n",
    "    answer = ground_truth_map.get(query, \"\")\n",
    "\n",
    "    if query and reply and answer:\n",
    "        chat_completion = llm.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"\"\"Evaluate the semantic similarity between the following two answers with respect to some question. Output only a single floating-point number between 0 and 1, where 0 indicates no similarity and 1 indicates identical meaning. Respond with only the number:\n",
    "Question: {query}\n",
    "Text A: {answer}\n",
    "Text B: {reply}\"\"\"\n",
    "                }\n",
    "            ],\n",
    "            model=\"llama-3.1-70b-versatile\",\n",
    "            stream=False,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            score = float(chat_completion.choices[0].message.content.strip())\n",
    "            LLM_Score += score\n",
    "        except ValueError:\n",
    "            print(\"Invalid LLM response for query:\", query)\n",
    "\n",
    "# Calculate and print average LLM similarity score\n",
    "average_llm_score = LLM_Score / count if count else 0\n",
    "print(f\"Average LLM Similarity Score: {average_llm_score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
